# train_tutor.yaml
model_name_or_path: meta-llama/Llama-2-7b-hf
dataset: tutor_dataset.json
output_dir: ./tutor_programacion_model

finetuning_type: lora
lora_target: q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj

template: llama2

per_device_train_batch_size: 2  
gradient_accumulation_steps: 4
lr_scheduler_type: cosine
learning_rate: 2e-4
num_train_epochs: 3
max_grad_norm: 1.0
warmup_steps: 100

fp16: true
quantization_bit: 4  
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.1

logging_steps: 10
save_steps: 100
eval_steps: 100
evaluation_strategy: steps
save_total_limit: 3

cutoff_len: 1024
overwrite_cache: true